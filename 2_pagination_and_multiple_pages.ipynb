{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0079349b",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6456b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/250.9 kB 245.8 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 92.2/250.9 kB 476.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/250.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/250.9 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 250.9/250.9 kB 768.9 kB/s eta 0:00:00\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\irmin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3c83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebf018",
   "metadata": {},
   "source": [
    "## 1) Pagination and Scraping Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302ab39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/catalogue/category/books_1/page-{}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5124ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed.\n",
      "Page 2 processed.\n",
      "Page 3 processed.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://books.toscrape.com/catalogue/category/books_1/page-{}.html\"\n",
    "product_list = []\n",
    "\n",
    "# Loop through the first 3 pages\n",
    "for page in range(1, 4):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    products = soup.select(\"article.product_pod\")\n",
    "    \n",
    "    for product in products:\n",
    "        title = product.find(\"h3\").find(\"a\")[\"title\"]\n",
    "        price = product.find(\"p\", class_=\"price_color\").get_text()\n",
    "        image_rel = product.find(\"div\", class_=\"image_container\").find(\"img\")[\"src\"]\n",
    "        image_url = \"http://books.toscrape.com/\" + image_rel.replace(\"../\", \"\")\n",
    "        product_list.append({\n",
    "            \"title\": title,\n",
    "            \"price\": price,\n",
    "            \"image_url\": image_url\n",
    "        })\n",
    "    \n",
    "    # Brief pause between pages to simulate real browsing\n",
    "    time.sleep(1)\n",
    "    print(f\"Page {page} processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643cb505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-page scraping completed: 60 products saved to productos_multi.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"resultados/productos_multi.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"image_url\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"Multi-page scraping completed: {len(product_list)} products saved to productos_multi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbdbc8",
   "metadata": {},
   "source": [
    "## 2) Handling Errors and Common Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a82fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 47 processed.\n",
      "Page 48 processed.\n",
      "Page 49 processed.\n",
      "Page 50 processed.\n",
      "Error on page 51: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-51.html\n",
      "Error on page 52: 404 Client Error: Not Found for url: http://books.toscrape.com/catalogue/category/books_1/page-52.html\n"
     ]
    }
   ],
   "source": [
    "product_list = []\n",
    "\n",
    "for page in range(47, 53):  # Test with 6 pages\n",
    "    url = base_url.format(page)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises error for 400 or 500 status codes\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        products = soup.select(\"article.product_pod\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error on page {page}: {e}\")\n",
    "        continue  # Continue with next iteration\n",
    "\n",
    "    for product in products:\n",
    "        try:\n",
    "            title = product.find(\"h3\").find(\"a\")[\"title\"]\n",
    "            price = product.find(\"p\", class_=\"price_color\").get_text()\n",
    "            image_rel = product.find(\"div\", class_=\"image_container\").find(\"img\")[\"src\"]\n",
    "            image_url = \"http://books.toscrape.com/\" + image_rel.replace(\"../\", \"\")\n",
    "            product_list.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"image_url\": image_url\n",
    "            })\n",
    "        except Exception as ex:\n",
    "            print(\"Error extracting data from a product:\", ex)\n",
    "    time.sleep(1)\n",
    "    print(f\"Page {page} processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fac6065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed with error handling: 80 products saved to productos_con_errores.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"resultados/productos_con_errores.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"image_url\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"Scraping completed with error handling: {len(product_list)} products saved to productos_con_errores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8bfc3",
   "metadata": {},
   "source": [
    "## 3) Best Practices: Headers, Timing, and Scraping Ethics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bb355",
   "metadata": {},
   "source": [
    "### üìú What is robots.txt?\n",
    "\n",
    "It's a file that websites place at their root (https://site.com/robots.txt) to indicate which parts of the site can or cannot be explored by bots. Although it's not a \"law\" (it doesn't technically prevent it), it's an ethical norm to respect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16734e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a header\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Mobile Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cea64c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/catalogue/category/books_1/page-{}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7779449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed with a pause of 2.76 seconds.\n",
      "Page 2 processed with a pause of 1.44 seconds.\n",
      "Page 3 processed with a pause of 2.98 seconds.\n"
     ]
    }
   ],
   "source": [
    "product_list = []\n",
    "\n",
    "for page in range(1, 4):\n",
    "    url = base_url.format(page)\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        products = soup.select(\"article.product_pod\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error on page {page}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for product in products:\n",
    "        try:\n",
    "            title = product.find(\"h3\").find(\"a\")[\"title\"]\n",
    "            price = product.find(\"p\", class_=\"price_color\").get_text()\n",
    "            image_rel = product.find(\"div\", class_=\"image_container\").find(\"img\")[\"src\"]\n",
    "            image_url = \"http://books.toscrape.com/\" + image_rel.replace(\"../\", \"\")\n",
    "            product_list.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"image_url\": image_url\n",
    "            })\n",
    "        except Exception as ex:\n",
    "            print(\"Error extracting data from a product:\", ex)\n",
    "    \n",
    "    # Random pause to mimic human behavior\n",
    "    sleep_time = random.uniform(1, 3)\n",
    "    time.sleep(sleep_time)\n",
    "    print(f\"Page {page} processed with a pause of {sleep_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1c9a4",
   "metadata": {},
   "source": [
    "### Save as CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355ca026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical scraping completed: 60 products saved to productos_eticos.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"resultados/productos_eticos.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"image_url\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"Ethical scraping completed: {len(product_list)} products saved to productos_eticos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6002908",
   "metadata": {},
   "source": [
    "### Save as JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "893ff90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported: 60 products in productos_final.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"resultados/productos_final.json\", \"w\", encoding=\"utf-8\") as jsonfile:\n",
    "    json.dump(product_list, jsonfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data exported: {len(product_list)} products in productos_final.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5e929",
   "metadata": {},
   "source": [
    "### Save as Excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6375a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical scraping completed: 60 products saved to productos_eticos.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to Excel\n",
    "df = pd.DataFrame(product_list)\n",
    "\n",
    "# Save as Excel file\n",
    "df.to_excel(\"resultados/productos_eticos.xlsx\", index=False)\n",
    "\n",
    "print(f\"Ethical scraping completed: {len(product_list)} products saved to productos_eticos.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dda7a",
   "metadata": {},
   "source": [
    "### Save to Google Form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accb12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71e8e8",
   "metadata": {},
   "source": [
    "### Example of form URL structure:\n",
    "https://docs.google.com/forms/d/e/1FAIpQLScFALHeZ6y-CJI_vy3f_78MFyNNGm4jz9ZZoLykEPbCpVdOrQ/viewform?usp=pp_url&entry.1204702772=pinocho&entry.464991896=1500&entry.406922421=www.pinocho.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38414691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Product 1 sent: A Light in the Attic\n",
      "‚úÖ Product 2 sent: Tipping the Velvet\n",
      "‚úÖ Product 3 sent: Soumission\n",
      "‚úÖ Product 4 sent: Sharp Objects\n",
      "‚úÖ Product 5 sent: Sapiens: A Brief History of Humankind\n"
     ]
    }
   ],
   "source": [
    "# Form URL\n",
    "url = \"https://docs.google.com/forms/d/e/1FAIpQLScFALHeZ6y-CJI_vy3f_78MFyNNGm4jz9ZZoLykEPbCpVdOrQ/formResponse\"\n",
    "\n",
    "# Headers to avoid 401 error\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "    \"Referer\": \"https://docs.google.com/forms/d/e/1FAIpQLScFALHeZ6y-CJI_vy3f_78MFyNNGm4jz9ZZoLykEPbCpVdOrQ/viewform\"\n",
    "}\n",
    "\n",
    "# Loop through and submit each product\n",
    "for i, product in enumerate(product_list[0:5], start=1):\n",
    "    payload = {\n",
    "        \"entry.1204702772\": product[\"title\"],        # field 1: title\n",
    "        \"entry.464991896\": product[\"price\"],        # field 2: price\n",
    "        \"entry.406922421\": product[\"image_url\"]     # field 3: image\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"‚úÖ Product {i} sent: {product['title']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error sending product {i} - Status code: {response.status_code}\")\n",
    "    \n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
